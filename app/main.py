# app/main.py

import requests
import gradio as gr
import json

# Local Ollama API
OLLAMA_API_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "hala-1.2b-arabic"  # The name you created with your Modelfile

def chat_with_model(prompt):
    """Send the user prompt to the local Ollama model and return its response."""
    try:
        # Request streaming response
        response = requests.post(OLLAMA_API_URL, json={
            "model": MODEL_NAME,
            "prompt": prompt
        }, stream=True)

        # Collect partial responses
        full_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode("utf-8"))
                    if "response" in data:
                        full_response += data["response"]
                except Exception:
                    continue  # Ignore malformed partial lines

        return full_response or "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."
    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}"


# Gradio interface
iface = gr.Interface(
    fn=chat_with_model,
    inputs=gr.Textbox(lines=5, placeholder="Ø§ÙƒØªØ¨ Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ø§Ù„Ù…Ø§Ù„ÙŠ Ù‡Ù†Ø§..."),
    outputs="text",
    title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ ğŸ¤–",
    description="ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Hala-1.2B Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¹Ø¨Ø± Ollama Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆÙƒØªØ§Ø¨Ø© ØªÙ‚Ø§Ø±ÙŠØ± Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
)

if __name__ == "__main__":
    iface.launch()
